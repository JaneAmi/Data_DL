{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAVNofqFf4Xc"
      },
      "source": [
        "## Question Answering\n",
        "In this exercise, you will experiment with one of NLPâ€™s exciting tasks - Question Answering!\n",
        "\n",
        "You will first evaluate a pre-trained model on Squad, a leading question-answering dataset, and evaluate its performance. Those with an approved access to GPUs in AWS or a different provider are encouraged to also fine-tune a base model on the Squad dataset.\n",
        "\n",
        "We will use HuggingFaceâ€™s Transformers, the leading package for NLP tasks using transformers. Your code should roughly follow the code of [this guide](https://huggingface.co/docs/transformers/tasks/question_answering) and [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).  \n",
        "\n",
        "(**Important Note:** The guide writes considerable amount of code to handle the case of context longer than the max input sequence. For simplicity, in your code, you should remove from the datasets all contexts longer than\n",
        "`max_length = 384`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnf5qRihf4Xj"
      },
      "source": [
        "This exercise utilizes large models. While we only fine-tune existing models, the time required for fine-tuning could be still large so you are not expected to make many runs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-GntWn9f4Xl"
      },
      "source": [
        "Install the transformers, datasets libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-14T06:32:53.448945Z",
          "start_time": "2022-05-14T06:32:08.776112Z"
        },
        "id": "RvrIs4Yff4Xm"
      },
      "outputs": [],
      "source": [
        "# ! pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zurZf64df4Xp"
      },
      "source": [
        "Import required libraries.  \n",
        "Make sure your version of Transformers is at least 4.11.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-18T19:21:15.737679Z",
          "start_time": "2022-05-18T19:21:15.663862Z"
        },
        "id": "_C7sFaTCf4Xq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/evgenia_k/miniconda3/envs/py4dp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.32.1\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siidvOb7f4Xr"
      },
      "source": [
        "We will use the ðŸ¤— [Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions load_dataset and load_metric.\n",
        "\n",
        "For our example here, we'll use version 1.1 of Stanford's [SQUAD dataset](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwwJH4Txf4Xs"
      },
      "source": [
        "Load the Squad v1.1 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-18T19:21:57.385828Z",
          "start_time": "2022-05-18T19:21:54.920317Z"
        },
        "id": "mYANBZRYf4Xt"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "datasets = load_dataset(\"squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wen5Th7sf4Xv"
      },
      "source": [
        "### Getting to know the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt5Evqrbf4Xv"
      },
      "source": [
        "The datasets object itself is DatasetDict, which contains one key for the training, validation and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-19T05:54:43.776922Z",
          "start_time": "2022-05-19T05:54:43.770922Z"
        },
        "id": "46tsqqQlf4Xw",
        "outputId": "beeddddf-3427-4773-9bb2-07b4619dd209"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7Roaw9sf4Xz"
      },
      "source": [
        "We can see the training, validation and test sets all have a column for the context, the question and the answers to those questions.\n",
        "\n",
        "To access an actual element, you need to select a split first, then give an index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-18T19:22:38.535553Z",
          "start_time": "2022-05-18T19:22:38.442614Z"
        },
        "id": "j6UNl4MNf4X0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '5733be284776f41900661182',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
              " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTTs-itwf4X0"
      },
      "source": [
        "Now, answer these questions:  \n",
        "\n",
        "What is the shortest context in the training dataset?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-18T19:22:57.607768Z",
          "start_time": "2022-05-18T19:22:46.559700Z"
        },
        "id": "NyYyMFsPf4X1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Australia: The event was held in Canberra, Australian Capital Territory on April 24, and covered around 16 km of Canberra's central areas, from Reconciliation Place to Commonwealth Park. Upon its arrival in Canberra, the Olympic flame was presented by Chinese officials to local Aboriginal elder Agnes Shea, of the Ngunnawal people. She, in turn, offered them a message stick, as a gift of peace and welcome. Hundreds of pro-Tibet protesters and thousands of Chinese students reportedly attended. Demonstrators and counter-demonstrators were kept apart by the Australian Federal Police. Preparations for the event were marred by a disagreement over the role of the Chinese flame attendants, with Australian and Chinese officials arguing publicly over their function and prerogatives during a press conference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "811"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(min(datasets[\"train\"]['context']))\n",
        "len(min(datasets[\"train\"]['context']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOIZefigf4X1"
      },
      "source": [
        "What is the longest answer in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-18T19:26:42.399583Z",
          "start_time": "2022-05-18T19:26:34.184166Z"
        },
        "id": "KCjelxZyf4X2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â€œThe roots of all our modern academic fields can be found within the pages of literature.â€  Literature in all its forms can be seen as written records, whether the literature itself be factual or fictional, it is still quite possible to decipher facts through things like charactersâ€™ actions and words or the authorsâ€™ style of writing and the intent behind the words. The plot is for more than just entertainment purposes; within it lies information about economics, psychology, science, religions, politics, cultures, and social depth. Studying and analyzing literature becomes very important in terms of learning about our history. Through the study of past literature we are able to learn about how society has evolved and about the societal norms during each of the different periods all throughout history. This can even help us to understand references made in more modern literature because authors often make references to Greek mythology and other old religious texts or historical moments. Not only is there literature written on each of the aforementioned topics themselves, and how they have evolved throughout history (like a book about the history of economics or a book about evolution and science, for example) but we can also learn about these things in fictional works. Authors often include historical moments in their works, like when Lord Byron talks about the Spanish and the French in â€˜â€˜Childe Haroldâ€™s Pilgrimage: Canto Iâ€™â€™ and expresses his opinions through his character Childe Harold. Through literature we are able to continuously uncover new information about history. It is easy to see how all academic fields have roots in literature. Information became easier to pass down from generation to generation once we began to write it down. Eventually everything was written down, from things like home remedies and cures for illness, or how to build shelter to traditions and religious practices. From there people were able to study literature, improve on ideas, further our knowledge, and academic fields such as the medical field or trades could be started. In much the same way as the literature that we study today continue to be updated as we continue to evolve and learn more and more.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2219"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(max(datasets[\"train\"]['context']))\n",
        "len(max(datasets[\"train\"]['context']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQxzx2hFf4X3"
      },
      "source": [
        "Is there a question that appears multiple times? What is the most common question?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Who won this season of Idol?', 6),\n",
              " ('Which Caribbean nation is in the top quartile of HDI (but missing IHDI)?',\n",
              "  6),\n",
              " (\"I couldn't could up with another question. But i need to fill this space because I can't submit the hit. \",\n",
              "  6),\n",
              " ('Who was a pop idol that started on American Idol?', 5),\n",
              " ('dd', 4),\n",
              " ('When was the region under Tibetan empire and the CHinese?', 3),\n",
              " ('Who took control of the regin in 710?', 3),\n",
              " ('Who restored Persian control of the region?', 3),\n",
              " ('What was the name of the region when it was the cultural center of Iran?',\n",
              "  3),\n",
              " ('What is the official name of Portugal?', 3)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(datasets[\"train\"]['question']).most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "373LB1tIf4X4"
      },
      "source": [
        "### HuggingFace transformersâ€™ tokenizers\n",
        "As a preprocessing step, the HuggingFace code tokenizes input sequences using a Tokenizer. Read more about tokenizers here:\n",
        "https://huggingface.co/docs/tokenizers/pipeline  \n",
        "https://huggingface.co/transformers/v3.0.2/preprocessing.html\n",
        "\n",
        "For this question, use the BERT tokenizer. The tokenizer sometimes breaks words into smaller chunks, so the number of tokens can be larger than the number of words.\n",
        "\n",
        "Using the first 1,000 context datapoints, print the 30 most common tokens by the tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "t7sWlrSAf4X4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/evgenia_k/miniconda3/envs/py4dp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [00:01<00:00, 7108.12 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_squad = datasets.map(preprocess_function, batched=True, remove_columns=datasets[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token: [PAD], Count: 183766\n",
            "Token: the, Count: 10653\n",
            "Token: ,, Count: 10558\n",
            "Token: ., Count: 7327\n",
            "Token: in, Count: 4986\n",
            "Token: and, Count: 4682\n",
            "Token: \", Count: 4500\n",
            "Token: of, Count: 4394\n",
            "Token: beyonce, Count: 2831\n",
            "Token: a, Count: 2772\n",
            "Token: ', Count: 2457\n",
            "Token: to, Count: 2280\n",
            "Token: s, Count: 2219\n",
            "Token: her, Count: 2021\n",
            "Token: [SEP], Count: 2000\n",
            "Token: -, Count: 1869\n",
            "Token: for, Count: 1762\n",
            "Token: on, Count: 1691\n",
            "Token: was, Count: 1538\n",
            "Token: with, Count: 1361\n",
            "Token: as, Count: 1329\n",
            "Token: at, Count: 1298\n",
            "Token: she, Count: 1171\n",
            "Token: ?, Count: 1014\n",
            "Token: [CLS], Count: 1000\n",
            "Token: by, Count: 851\n",
            "Token: is, Count: 814\n",
            "Token: album, Count: 796\n",
            "Token: that, Count: 751\n",
            "Token: (, Count: 701\n"
          ]
        }
      ],
      "source": [
        "# extract the first 1000 tokenized contexts\n",
        "first_1000 = tokenized_squad['train'].select(range(1000))\n",
        "\n",
        "# a counter to hold our token counts\n",
        "token_counter = Counter()\n",
        "\n",
        "for example in first_1000:\n",
        "    # convert token ids back to tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(example['input_ids'])\n",
        "    # update the counter\n",
        "    token_counter.update(tokens)\n",
        "\n",
        "# get the 30 most common tokens\n",
        "most_common_tokens = token_counter.most_common(30)\n",
        "\n",
        "for token, count in most_common_tokens:\n",
        "    print(f\"Token: {token}, Count: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-18T19:36:30.771751Z",
          "start_time": "2022-05-18T19:36:30.758022Z"
        },
        "id": "vmzITB8Kf4X5"
      },
      "source": [
        "### Load a pretrained Question Answering model\n",
        "In this section, you will use a model pretrained on the Squad dataset for question answering.  \n",
        "\n",
        "Choose a model you'd like to use.  \n",
        "You can see a list of available models here: https://huggingface.co/models?dataset=dataset:squad&sort=downloads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc8wYNOkf4X5"
      },
      "source": [
        "Load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "O52waqzcf4X6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/evgenia_k/miniconda3/envs/py4dp/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/Users/evgenia_k/miniconda3/envs/py4dp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-cased-distilled-squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ73bpRGf4X6"
      },
      "source": [
        "### Pretrained Model Error Analysis\n",
        "Here you will evaluate your modelâ€™s performance.\n",
        "\n",
        "Write code to manually review a few errors of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_validation_examples(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    example_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "        sample_idx = sample_map[i]\n",
        "        example_ids.append(examples[\"id\"][sample_idx])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        inputs[\"offset_mapping\"][i] = [\n",
        "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
        "        ]\n",
        "\n",
        "    inputs[\"example_id\"] = example_ids\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/evgenia_k/miniconda3/envs/py4dp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 5548.14 examples/s]\n"
          ]
        }
      ],
      "source": [
        "small_eval_set = datasets[\"validation\"].select(range(0, 500))\n",
        "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "eval_set = small_eval_set.map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"validation\"].column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "eval_set_for_model.set_format(\"torch\")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
        "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
        "# batch = {k: torch.tensor(eval_set[k]).to(device) for k in eval_set.keys()}\n",
        "trained_model = model.to(\n",
        "    device\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = trained_model(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_logits = outputs.start_logits.cpu().numpy()\n",
        "end_logits = outputs.end_logits.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "example_to_features = defaultdict(list)\n",
        "for idx, feature in enumerate(eval_set):\n",
        "    example_to_features[feature[\"example_id\"]].append(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "n_best = 20\n",
        "max_answer_length = 30\n",
        "predicted_answers = []\n",
        "\n",
        "for example in small_eval_set:\n",
        "    example_id = example[\"id\"]\n",
        "    context = example[\"context\"]\n",
        "    answers = []\n",
        "\n",
        "    for feature_index in example_to_features[example_id]:\n",
        "        start_logit = start_logits[feature_index]\n",
        "        end_logit = end_logits[feature_index]\n",
        "        offsets = eval_set[\"offset_mapping\"][feature_index]\n",
        "\n",
        "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "        for start_index in start_indexes:\n",
        "            for end_index in end_indexes:\n",
        "                # Skip answers that are not fully in the context\n",
        "                if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                    continue\n",
        "                # Skip answers with a length that is either < 0 or > max_answer_length.\n",
        "                if (\n",
        "                    end_index < start_index\n",
        "                    or end_index - start_index + 1 > max_answer_length\n",
        "                ):\n",
        "                    continue\n",
        "\n",
        "                answers.append(\n",
        "                    {\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "theoretical_answers = [\n",
        "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted answer: Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\n",
            "Correct answers: ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"]\n",
            "\n",
            "Predicted answer: Carolina Panthers\n",
            "Correct answers: ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
            "\n",
            "Predicted answer: golden anniversary\n",
            "Correct answers: ['\"golden anniversary\"', 'gold-themed', '\"golden anniversary']\n",
            "\n",
            "Predicted answer: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference\n",
            "Correct answers: ['American Football Conference', 'American Football Conference', 'American Football Conference']\n",
            "\n",
            "Predicted answer: golden anniversary\n",
            "Correct answers: ['\"golden anniversary\"', 'gold-themed', 'gold']\n",
            "\n",
            "Predicted answer: Santa Clara, California\n",
            "Correct answers: ['Santa Clara', 'Santa Clara', 'Santa Clara']\n",
            "\n",
            "Predicted answer: Santa Clara, California\n",
            "Correct answers: ['Santa Clara', 'Santa Clara', 'Santa Clara']\n",
            "\n",
            "Predicted answer: Carolina Panthers\n",
            "Correct answers: ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
            "\n",
            "Predicted answer: Super Bowl 50\n",
            "Correct answers: ['Super Bowl', 'Super Bowl', 'Super Bowl']\n",
            "\n",
            "Predicted answer: Arizona Cardinals\n",
            "Correct answers: ['New England Patriots', 'the New England Patriots', 'New England Patriots']\n",
            "\n",
            "Predicted answer: 15â€“1 record\n",
            "Correct answers: ['12â€“4', '12â€“4', '12â€“4']\n",
            "\n",
            "Predicted answer: eight\n",
            "Correct answers: ['2', 'second', 'second']\n",
            "\n",
            "Predicted answer: Arizona Cardinals\n",
            "Correct answers: ['New England Patriots', 'the New England Patriots', 'New England Patriots']\n",
            "\n",
            "Predicted answer: Arizona Cardinals\n",
            "Correct answers: ['New England Patriots', 'the New England Patriots', 'New England Patriots']\n",
            "\n",
            "Predicted answer: Super Bowl 50\n",
            "Correct answers: ['Super Bowl XLVII', 'Super Bowl XLVII', 'XLVII']\n",
            "\n",
            "Predicted answer: BeyoncÃ© and Bruno Mars\n",
            "Correct answers: ['BeyoncÃ©', 'BeyoncÃ©', 'BeyoncÃ©']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# show wrong answers\n",
        "i = 0\n",
        "\n",
        "for p_answ, t_answ in zip(predicted_answers, theoretical_answers):\n",
        "    if p_answ['prediction_text'] not in t_answ['answers']['text']:\n",
        "        print(\"Predicted answer:\", p_answ['prediction_text'])\n",
        "        print(\"Correct answers:\", t_answ['answers']['text'])\n",
        "        print()\n",
        "        i+=1\n",
        "    \n",
        "    if i > 15:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'exact_match': 84.0, 'f1': 87.00842373312962}"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"squad\")\n",
        "metric.compute(predictions=predicted_answers, references=theoretical_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd6WWZASf4X7"
      },
      "source": [
        "Do you see a pattern there? Is there any hypothesis you form for cases where the model fails?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy17O4Jgf4X7"
      },
      "source": [
        "**Pattern:**\n",
        "\n",
        "The model tends to provide overly specific or incorrect answers. It often includes additional context or selects similar but incorrect entities.  Also, it misses some punctuation signs which appear in the correct answers\n",
        "\n",
        "**Hypothesis:**\n",
        "\n",
        "- It sometimes confuses similar entities, suggesting difficulties in distinguishing closely related concepts or a bias towards more frequently occurring entities in the training set;\n",
        "\n",
        "- The model sometimes omits necessary punctuation in answers, affecting metrics reliant on exact string matches, which may stem from tokenization handling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA22s0e4f4X8"
      },
      "source": [
        "Write code that runs inference and outputs the predicted answer to a context and question texts typed by the user. We recommend that you use ipywidgets for interactivity:  \n",
        "https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Basics.html\n",
        "\n",
        "Use the award-winning GUI youâ€™ve just created, to try to manually poke holes in the model. Try to characterize the cases your model mishandles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5vwbKwtf4X8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEggDivUf4X9"
      },
      "source": [
        "Next, evaluate your modelâ€™s performance for different lengths of input text and of answer length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpoZqLdxf4X9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdkcNpZVf4X9"
      },
      "source": [
        "BONUS: Can you think of other axes that would be interesting to use to evaluate your model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ntZzcrlf4X-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGRa8h0Df4X-"
      },
      "source": [
        "### [Advanced] Fine-tune a Model\n",
        "Here, you will fine-tune a base model to the Squad dataset, and evaluate its performance.\n",
        "\n",
        "What metric do you find suited? Why?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrRi5Xv2f4X-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmteE8wAf4X_"
      },
      "source": [
        "Train the model to fine-tune on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-14T08:24:02.833322Z",
          "start_time": "2022-05-14T08:23:58.957315Z"
        },
        "id": "MxYXwKEof4YB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo6C_wutf4YK"
      },
      "source": [
        "Write below your train and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fll04-GSf4YK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcX9CaHzf4YM"
      },
      "source": [
        "## Recommended Resources\n",
        "For an open discussion on Question Answering related topics, you are very encouraged to watch this workshop: https://www.youtube.com/watch?v=Ihgk8kGLpIE\n",
        "\n",
        "This screencast uses T5 on a different Q&A dataset: https://www.youtube.com/watch?v=_l2wJb3QPdk\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKepdA-of4YM"
      },
      "source": [
        "That's it - good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5jiKDlvf4YN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
