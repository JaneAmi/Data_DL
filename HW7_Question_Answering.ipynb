{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAVNofqFf4Xc"
      },
      "source": [
        "## Question Answering\n",
        "In this exercise, you will experiment with one of NLPâ€™s exciting tasks - Question Answering!\n",
        "\n",
        "You will first evaluate a pre-trained model on Squad, a leading question-answering dataset, and evaluate its performance. Those with an approved access to GPUs in AWS or a different provider are encouraged to also fine-tune a base model on the Squad dataset.\n",
        "\n",
        "We will use HuggingFaceâ€™s Transformers, the leading package for NLP tasks using transformers. Your code should roughly follow the code of [this guide](https://huggingface.co/docs/transformers/tasks/question_answering) and [this notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).  \n",
        "\n",
        "(**Important Note:** The guide writes considerable amount of code to handle the case of context longer than the max input sequence. For simplicity, in your code, you should remove from the datasets all contexts longer than\n",
        "`max_length = 384`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnf5qRihf4Xj"
      },
      "source": [
        "This exercise utilizes large models. While we only fine-tune existing models, the time required for fine-tuning could be still large so you are not expected to make many runs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-GntWn9f4Xl"
      },
      "source": [
        "Install the transformers, datasets libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-14T06:32:53.448945Z",
          "start_time": "2022-05-14T06:32:08.776112Z"
        },
        "id": "RvrIs4Yff4Xm"
      },
      "outputs": [],
      "source": [
        "# ! pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zurZf64df4Xp"
      },
      "source": [
        "Import required libraries.  \n",
        "Make sure your version of Transformers is at least 4.11.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-18T19:21:15.737679Z",
          "start_time": "2022-05-18T19:21:15.663862Z"
        },
        "id": "_C7sFaTCf4Xq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.32.1\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siidvOb7f4Xr"
      },
      "source": [
        "We will use the ðŸ¤— [Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions load_dataset and load_metric.\n",
        "\n",
        "For our example here, we'll use version 1.1 of Stanford's [SQUAD dataset](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwwJH4Txf4Xs"
      },
      "source": [
        "Load the Squad v1.1 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-18T19:21:57.385828Z",
          "start_time": "2022-05-18T19:21:54.920317Z"
        },
        "id": "mYANBZRYf4Xt"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "datasets = load_dataset(\"squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wen5Th7sf4Xv"
      },
      "source": [
        "### Getting to know the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt5Evqrbf4Xv"
      },
      "source": [
        "The datasets object itself is DatasetDict, which contains one key for the training, validation and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-19T05:54:43.776922Z",
          "start_time": "2022-05-19T05:54:43.770922Z"
        },
        "id": "46tsqqQlf4Xw",
        "outputId": "beeddddf-3427-4773-9bb2-07b4619dd209"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7Roaw9sf4Xz"
      },
      "source": [
        "We can see the training, validation and test sets all have a column for the context, the question and the answers to those questions.\n",
        "\n",
        "To access an actual element, you need to select a split first, then give an index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-18T19:22:38.535553Z",
          "start_time": "2022-05-18T19:22:38.442614Z"
        },
        "id": "j6UNl4MNf4X0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '5733be284776f41900661182',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
              " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTTs-itwf4X0"
      },
      "source": [
        "Now, answer these questions:  \n",
        "\n",
        "What is the shortest context in the training dataset?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-18T19:22:57.607768Z",
          "start_time": "2022-05-18T19:22:46.559700Z"
        },
        "id": "NyYyMFsPf4X1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shortest context: Meanwhile, the USSR continued briefly trying to perfect their N1 rocket, finally canceling it in 1976, after two more launch failures in 1971 and 1972.\n",
            "Length of shortest context: 151\n"
          ]
        }
      ],
      "source": [
        "shortest_context = min(datasets[\"train\"]['context'], key=len)\n",
        "print(\"Shortest context:\", shortest_context)\n",
        "print(\"Length of shortest context:\", len(shortest_context))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOIZefigf4X1"
      },
      "source": [
        "What is the longest answer in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-18T19:26:42.399583Z",
          "start_time": "2022-05-18T19:26:34.184166Z"
        },
        "id": "KCjelxZyf4X2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longest answer: that the sudden shift of a huge quantity of water into the region could have relaxed the tension between the two sides of the fault, allowing them to move apart, and could have increased the direct pressure on it, causing a violent rupture\n",
            "Length of longest answer: 239\n"
          ]
        }
      ],
      "source": [
        "longest_answer = max(datasets[\"train\"]['answers'], key=lambda ans: len(max(ans['text'], key=len)))\n",
        "\n",
        "print(\"Longest answer:\", max(longest_answer['text'], key=len))\n",
        "print(\"Length of longest answer:\", len(max(longest_answer['text'], key=len)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQxzx2hFf4X3"
      },
      "source": [
        "Is there a question that appears multiple times? What is the most common question?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Who won this season of Idol?', 6),\n",
              " ('Which Caribbean nation is in the top quartile of HDI (but missing IHDI)?',\n",
              "  6),\n",
              " (\"I couldn't could up with another question. But i need to fill this space because I can't submit the hit. \",\n",
              "  6),\n",
              " ('Who was a pop idol that started on American Idol?', 5),\n",
              " ('dd', 4),\n",
              " ('When was the region under Tibetan empire and the CHinese?', 3),\n",
              " ('Who took control of the regin in 710?', 3),\n",
              " ('Who restored Persian control of the region?', 3),\n",
              " ('What was the name of the region when it was the cultural center of Iran?',\n",
              "  3),\n",
              " ('What is the official name of Portugal?', 3)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(datasets[\"train\"]['question']).most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are three most common quations that appear 6 times in the dataset:\n",
        "- 'Who won this season of Idol?'\n",
        "- 'Which Caribbean nation is in the top quartile of HDI (but missing IHDI)?'\n",
        "- \"I couldn't could up with another question. But i need to fill this space because I can't submit the hit. \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "373LB1tIf4X4"
      },
      "source": [
        "### HuggingFace transformersâ€™ tokenizers\n",
        "As a preprocessing step, the HuggingFace code tokenizes input sequences using a Tokenizer. Read more about tokenizers here:\n",
        "https://huggingface.co/docs/tokenizers/pipeline  \n",
        "https://huggingface.co/transformers/v3.0.2/preprocessing.html\n",
        "\n",
        "For this question, use the BERT tokenizer. The tokenizer sometimes breaks words into smaller chunks, so the number of tokens can be larger than the number of words.\n",
        "\n",
        "Using the first 1,000 context datapoints, print the 30 most common tokens by the tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "t7sWlrSAf4X4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "851fdc62d11642249e24f4a5882cf881",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_squad = datasets.map(preprocess_function, batched=True, remove_columns=datasets[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token: [PAD], Count: 183766\n",
            "Token: the, Count: 10653\n",
            "Token: ,, Count: 10558\n",
            "Token: ., Count: 7327\n",
            "Token: in, Count: 4986\n",
            "Token: and, Count: 4682\n",
            "Token: \", Count: 4500\n",
            "Token: of, Count: 4394\n",
            "Token: beyonce, Count: 2831\n",
            "Token: a, Count: 2772\n",
            "Token: ', Count: 2457\n",
            "Token: to, Count: 2280\n",
            "Token: s, Count: 2219\n",
            "Token: her, Count: 2021\n",
            "Token: [SEP], Count: 2000\n",
            "Token: -, Count: 1869\n",
            "Token: for, Count: 1762\n",
            "Token: on, Count: 1691\n",
            "Token: was, Count: 1538\n",
            "Token: with, Count: 1361\n",
            "Token: as, Count: 1329\n",
            "Token: at, Count: 1298\n",
            "Token: she, Count: 1171\n",
            "Token: ?, Count: 1014\n",
            "Token: [CLS], Count: 1000\n",
            "Token: by, Count: 851\n",
            "Token: is, Count: 814\n",
            "Token: album, Count: 796\n",
            "Token: that, Count: 751\n",
            "Token: (, Count: 701\n"
          ]
        }
      ],
      "source": [
        "# extract the first 1000 tokenized contexts\n",
        "first_1000 = tokenized_squad['train'].select(range(1000))\n",
        "\n",
        "# a counter to hold our token counts\n",
        "token_counter = Counter()\n",
        "\n",
        "for example in first_1000:\n",
        "    # convert token ids back to tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(example['input_ids'])\n",
        "    # update the counter\n",
        "    token_counter.update(tokens)\n",
        "\n",
        "# get the 30 most common tokens\n",
        "most_common_tokens = token_counter.most_common(30)\n",
        "\n",
        "for token, count in most_common_tokens:\n",
        "    print(f\"Token: {token}, Count: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-18T19:36:30.771751Z",
          "start_time": "2022-05-18T19:36:30.758022Z"
        },
        "id": "vmzITB8Kf4X5"
      },
      "source": [
        "### Load a pretrained Question Answering model\n",
        "In this section, you will use a model pretrained on the Squad dataset for question answering.  \n",
        "\n",
        "Choose a model you'd like to use.  \n",
        "You can see a list of available models here: https://huggingface.co/models?dataset=dataset:squad&sort=downloads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc8wYNOkf4X5"
      },
      "source": [
        "Load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "O52waqzcf4X6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/evgenia_k/miniconda3/envs/py4dp/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/Users/evgenia_k/miniconda3/envs/py4dp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ73bpRGf4X6"
      },
      "source": [
        "### Pretrained Model Error Analysis\n",
        "Here you will evaluate your modelâ€™s performance.\n",
        "\n",
        "Write code to manually review a few errors of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_validation_examples(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    example_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "        sample_idx = sample_map[i]\n",
        "        example_ids.append(examples[\"id\"][sample_idx])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        inputs[\"offset_mapping\"][i] = [\n",
        "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
        "        ]\n",
        "\n",
        "    inputs[\"example_id\"] = example_ids\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "small_eval_set = datasets[\"validation\"].select(range(0, 500))\n",
        "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "eval_set = small_eval_set.map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"validation\"].column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "eval_set_for_model.set_format(\"torch\")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
        "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
        "# batch = {k: torch.tensor(eval_set[k]).to(device) for k in eval_set.keys()}\n",
        "trained_model = model.to(\n",
        "    device\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = trained_model(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_logits = outputs.start_logits.cpu().numpy()\n",
        "end_logits = outputs.end_logits.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "example_to_features = defaultdict(list)\n",
        "for idx, feature in enumerate(eval_set):\n",
        "    example_to_features[feature[\"example_id\"]].append(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "n_best = 20\n",
        "max_answer_length = 30\n",
        "predicted_answers = []\n",
        "\n",
        "for example in small_eval_set:\n",
        "    example_id = example[\"id\"]\n",
        "    context = example[\"context\"]\n",
        "    answers = []\n",
        "\n",
        "    for feature_index in example_to_features[example_id]:\n",
        "        start_logit = start_logits[feature_index]\n",
        "        end_logit = end_logits[feature_index]\n",
        "        offsets = eval_set[\"offset_mapping\"][feature_index]\n",
        "\n",
        "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "        for start_index in start_indexes:\n",
        "            for end_index in end_indexes:\n",
        "                # Skip answers that are not fully in the context\n",
        "                if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                    continue\n",
        "                # Skip answers with a length that is either < 0 or > max_answer_length.\n",
        "                if (\n",
        "                    end_index < start_index\n",
        "                    or end_index - start_index + 1 > max_answer_length\n",
        "                ):\n",
        "                    continue\n",
        "\n",
        "                answers.append(\n",
        "                    {\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})\n",
        "\n",
        "theoretical_answers = [\n",
        "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "theoretical_answers = [\n",
        "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted answer: Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\n",
            "Correct answers: ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"]\n",
            "\n",
            "Predicted answer: Carolina Panthers\n",
            "Correct answers: ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
            "\n",
            "Predicted answer: golden anniversary\n",
            "Correct answers: ['\"golden anniversary\"', 'gold-themed', '\"golden anniversary']\n",
            "\n",
            "Predicted answer: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference\n",
            "Correct answers: ['American Football Conference', 'American Football Conference', 'American Football Conference']\n",
            "\n",
            "Predicted answer: golden anniversary\n",
            "Correct answers: ['\"golden anniversary\"', 'gold-themed', 'gold']\n",
            "\n",
            "Predicted answer: Santa Clara, California\n",
            "Correct answers: ['Santa Clara', 'Santa Clara', 'Santa Clara']\n",
            "\n",
            "Predicted answer: Santa Clara, California\n",
            "Correct answers: ['Santa Clara', 'Santa Clara', 'Santa Clara']\n",
            "\n",
            "Predicted answer: Carolina Panthers\n",
            "Correct answers: ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
            "\n",
            "Predicted answer: Super Bowl 50\n",
            "Correct answers: ['Super Bowl', 'Super Bowl', 'Super Bowl']\n",
            "\n",
            "Predicted answer: Arizona Cardinals\n",
            "Correct answers: ['New England Patriots', 'the New England Patriots', 'New England Patriots']\n",
            "\n",
            "Predicted answer: 15â€“1 record\n",
            "Correct answers: ['12â€“4', '12â€“4', '12â€“4']\n",
            "\n",
            "Predicted answer: eight\n",
            "Correct answers: ['2', 'second', 'second']\n",
            "\n",
            "Predicted answer: Arizona Cardinals\n",
            "Correct answers: ['New England Patriots', 'the New England Patriots', 'New England Patriots']\n",
            "\n",
            "Predicted answer: Arizona Cardinals\n",
            "Correct answers: ['New England Patriots', 'the New England Patriots', 'New England Patriots']\n",
            "\n",
            "Predicted answer: Super Bowl 50\n",
            "Correct answers: ['Super Bowl XLVII', 'Super Bowl XLVII', 'XLVII']\n",
            "\n",
            "Predicted answer: BeyoncÃ© and Bruno Mars\n",
            "Correct answers: ['BeyoncÃ©', 'BeyoncÃ©', 'BeyoncÃ©']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# show n wrong answers\n",
        "n = 15\n",
        "i = 0\n",
        "\n",
        "for p_answ, t_answ in zip(predicted_answers, theoretical_answers):\n",
        "    if p_answ['prediction_text'] not in t_answ['answers']['text']:\n",
        "        print(\"Predicted answer:\", p_answ['prediction_text'])\n",
        "        print(\"Correct answers:\", t_answ['answers']['text'])\n",
        "        print()\n",
        "        i+=1\n",
        "    \n",
        "    if i > n:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'exact_match': 84.0, 'f1': 87.00842373312962}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"squad\")\n",
        "metric.compute(predictions=predicted_answers, references=theoretical_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd6WWZASf4X7"
      },
      "source": [
        "Do you see a pattern there? Is there any hypothesis you form for cases where the model fails?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy17O4Jgf4X7"
      },
      "source": [
        "**Pattern:**\n",
        "\n",
        "The model tends to provide overly specific or incorrect answers. It often includes additional context or selects similar but incorrect entities.  Also, it misses some punctuation signs which appear in the correct answers\n",
        "\n",
        "**Hypothesis:**\n",
        "\n",
        "- It sometimes confuses similar entities, suggesting difficulties in distinguishing closely related concepts or a bias towards more frequently occurring entities in the training set;\n",
        "\n",
        "- The model sometimes omits necessary punctuation in answers, affecting metrics reliant on exact string matches, which may stem from tokenization handling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA22s0e4f4X8"
      },
      "source": [
        "Write code that runs inference and outputs the predicted answer to a context and question texts typed by the user. We recommend that you use ipywidgets for interactivity:  \n",
        "https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Basics.html\n",
        "\n",
        "Use the award-winning GUI youâ€™ve just created, to try to manually poke holes in the model. Try to characterize the cases your model mishandles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "U5vwbKwtf4X8"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "question_answerer = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72f285635d9e42ebaa93630c08404813",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Textarea(value='Context here...', description='Context:', layout=Layout(height='200px', width='400px'), placehâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5345120fcc8c4d73b6b5df7b7a2e1e2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Text(value='Question here...', description='Question:', layout=Layout(width='400px'), placeholder='Type somethâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9792778ec7954923bedda2b49b818124",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Button(description='Get Answer', style=ButtonStyle())"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a5fb02912c34599a3348b370e5a5308",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Label(value='Answer will appear here...', layout=Layout(height='100px', width='400px'), style=LabelStyle(descrâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.to('cpu')\n",
        "\n",
        "def get_answer(question, context):\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits + 1\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores)\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    # return question_answerer(question = question, context = context)[\"answer\"]\n",
        "    return answer\n",
        "\n",
        "# Widget UI\n",
        "context_text = widgets.Textarea(\n",
        "    value='Context here...',\n",
        "    placeholder='Type something',\n",
        "    description='Context:',\n",
        "    layout={'width': '400px', 'height': '200px'},\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "question_text = widgets.Text(\n",
        "    value='Question here...',\n",
        "    placeholder='Type something',\n",
        "    description='Question:',\n",
        "    layout={'width': '400px'},\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "answer_output = widgets.Label(\n",
        "    value=\"Answer will appear here...\",\n",
        "    layout={'width': '400px', 'height': '100px'},\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    # display the answer\n",
        "    answer = get_answer(question_text.value, context_text.value)\n",
        "    answer_output.value = \"Answer: \" + answer\n",
        "\n",
        "submit_button = widgets.Button(description=\"Get Answer\")\n",
        "submit_button.on_click(on_button_clicked)\n",
        "\n",
        "# Display widgets\n",
        "display(context_text, question_text, submit_button, answer_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEggDivUf4X9"
      },
      "source": [
        "Next, evaluate your modelâ€™s performance for different lengths of input text and of answer length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ZpoZqLdxf4X9"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# n_best = 20\n",
        "# max_answer_length = 30\n",
        "# predicted_answers = []\n",
        "\n",
        "# for example in small_eval_set:\n",
        "#     example_id = example[\"id\"]\n",
        "#     context = example[\"context\"]\n",
        "#     answers = []\n",
        "\n",
        "#     for feature_index in example_to_features[example_id]:\n",
        "#         start_logit = start_logits[feature_index]\n",
        "#         end_logit = end_logits[feature_index]\n",
        "#         offsets = eval_set[\"offset_mapping\"][feature_index]\n",
        "\n",
        "#         start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "#         end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "#         for start_index in start_indexes:\n",
        "#             for end_index in end_indexes:\n",
        "#                 # Skip answers that are not fully in the context\n",
        "#                 if offsets[start_index] is None or offsets[end_index] is None:\n",
        "#                     continue\n",
        "#                 # Skip answers with a length that is either < 0 or > max_answer_length.\n",
        "#                 if (\n",
        "#                     end_index < start_index\n",
        "#                     or end_index - start_index + 1 > max_answer_length\n",
        "#                 ):\n",
        "#                     continue\n",
        "\n",
        "#                 answers.append(\n",
        "#                     {\n",
        "#                         \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "#                         \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "#                     }\n",
        "#                 )\n",
        "\n",
        "#     best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "#     predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})\n",
        "\n",
        "# theoretical_answers = [\n",
        "#     {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdkcNpZVf4X9"
      },
      "source": [
        "BONUS: Can you think of other axes that would be interesting to use to evaluate your model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ntZzcrlf4X-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGRa8h0Df4X-"
      },
      "source": [
        "### [Advanced] Fine-tune a Model\n",
        "Here, you will fine-tune a base model to the Squad dataset, and evaluate its performance.\n",
        "\n",
        "What metric do you find suited? Why?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrRi5Xv2f4X-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmteE8wAf4X_"
      },
      "source": [
        "Train the model to fine-tune on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-05-14T08:24:02.833322Z",
          "start_time": "2022-05-14T08:23:58.957315Z"
        },
        "id": "MxYXwKEof4YB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo6C_wutf4YK"
      },
      "source": [
        "Write below your train and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fll04-GSf4YK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcX9CaHzf4YM"
      },
      "source": [
        "## Recommended Resources\n",
        "For an open discussion on Question Answering related topics, you are very encouraged to watch this workshop: https://www.youtube.com/watch?v=Ihgk8kGLpIE\n",
        "\n",
        "This screencast uses T5 on a different Q&A dataset: https://www.youtube.com/watch?v=_l2wJb3QPdk\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKepdA-of4YM"
      },
      "source": [
        "That's it - good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5jiKDlvf4YN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
